{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ukbang/Build_with_AI_250405/blob/main/3_ReportAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgWF2j10oSEh"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langgraph langchain-google-genai langchain-core langchain-community langchain-experimental fpdf pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jLETHnXnKBh"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableConfig\n",
        "from langchain.schema import AIMessage\n",
        "from typing import Annotated, Literal\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.tools import tool\n",
        "from langchain_community.agent_toolkits import FileManagementToolkit\n",
        "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
        "from pydantic import BaseModel, Field\n",
        "from fpdf import FPDF\n",
        "import random\n",
        "import pdfplumber\n",
        "import os\n",
        "import requests\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtLQpBZQIHsa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "google_api_key = \"\" # @param {\"type\":\"string\"}\n",
        "tavily_api_key = \"\" # @param {\"type\":\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8azdfq-l-ps"
      },
      "outputs": [],
      "source": [
        "os.environ[\"google_api_key\"] = google_api_key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
        "os.environ[\"TAVILY_API_KEY\"] = tavily_api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuwqB6S6nKBl"
      },
      "outputs": [],
      "source": [
        "# 그래프 상태 정의\n",
        "\n",
        "class State(TypedDict):\n",
        "    query : Annotated[str, \"User Question\"]\n",
        "    answer : Annotated[str, \"LLM response\"]\n",
        "    messages : Annotated[list, add_messages]\n",
        "    tool_call : Annotated[dict, \"Tool Call Result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-CNSwr0qnsi"
      },
      "outputs": [],
      "source": [
        "# LLM 정의\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro-exp-03-25\",\n",
        "                             temperature=0,\n",
        "                             convert_system_message_to_human=True)\n",
        "\n",
        "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
        "#                              temperature=0,\n",
        "#                              convert_system_message_to_human=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNMa89_7nKBm"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def read_pdf(file_path: str):\n",
        "    \"\"\"\n",
        "    PDF 파일에서 텍스트를 추출하는 도구입니다.\n",
        "    표 형식 또는 일반 텍스트가 포함된 PDF를 읽고 문자열로 반환합니다.\n",
        "\n",
        "    file_path 예시: './report.pdf'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "        return text.strip() if text.strip() else \"❌ PDF에서 텍스트를 추출할 수 없습니다.\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ PDF 읽기 오류: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqkLXOw6nKBn"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def write_pdf(content: str, filename: str = \"output.pdf\", summary: bool =True):\n",
        "    \"\"\"\n",
        "    텍스트를 PDF 파일로 저장하는 도구입니다.\n",
        "    PDF형태의 문서로 만들어야할 때 이 도구를 사용하세요.\n",
        "    \"\"\"\n",
        "\n",
        "    if summary:\n",
        "        prompt = PromptTemplate.from_template(\"\"\"\n",
        "                당신은 보고서를 작성하는 어시스턴트입니다. 당신에겐 문서 모음이 제공되고 이를 잘 분석하여 보고서를 작성하여야 합니다.\n",
        "                아래의 content는 문서 모음입니다. 문서의 제목, 본문을 잘 판단하고 정리하여 요약합니다.\n",
        "                항상 구조화된 출력을 제공하세요.\n",
        "                항상 마지막엔 인사이트도 첨부합니다.\n",
        "\n",
        "                content : {content}\n",
        "                \"\"\")\n",
        "\n",
        "        chain = prompt | llm\n",
        "\n",
        "        content = chain.invoke({\"content\":content}).content\n",
        "\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    font_url = \"https://github.com/google/fonts/raw/main/ofl/notosanskr/NotoSansKR%5Bwght%5D.ttf\"\n",
        "    font_path = \"./fonts/NotoSansKR.ttf\"\n",
        "\n",
        "    try:\n",
        "        os.mkdir(\"./fonts/\")\n",
        "        response = requests.get(font_url)\n",
        "        with open(font_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "\n",
        "    font_path = \"./fonts/NotoSansKR.ttf\"  # <-- 여기에 실제 폰트 파일이 있어야 함\n",
        "\n",
        "    try:\n",
        "        pdf.add_font(\"NotoSans\", \"\", font_path, uni=True)\n",
        "        pdf.set_font(\"NotoSans\", size=12)\n",
        "    except:\n",
        "        raise ValueError(\"한글 폰트를 등록할 수 없습니다.\")\n",
        "\n",
        "    for line in content.split(\"\\n\"):\n",
        "        pdf.multi_cell(0, 10, line)\n",
        "    pdf.output(f\"./{filename}\")\n",
        "\n",
        "    return f\"{filename} 저장 완료\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKMUZEXPIHsc"
      },
      "outputs": [],
      "source": [
        "# 툴 정의\n",
        "# TavilySearchResults : 웹 검색 도구\n",
        "# PythonAstREPLTool : 파이썬 코드 실행 도구\n",
        "# write_pdf : pdf 생성 도구\n",
        "# read_pdf : pdf 읽기 도구\n",
        "# file_delete : 파일 삭제 도구\n",
        "# list_directory : 파일 목록 읽기 도구\n",
        "\n",
        "tools = [TavilySearchResults(max_results=10), PythonAstREPLTool(), write_pdf, read_pdf, *FileManagementToolkit(\n",
        "                                                                            selected_tools=[\"file_delete\",\"list_directory\"]).get_tools()]\n",
        "search_tool, code_tool, write_tool, read_tool, delete_tool, listdir_tool= tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmTiddGxyB6K"
      },
      "outputs": [],
      "source": [
        "# PDF 쓰기 도구 예시\n",
        "\n",
        "write_tool.invoke({\"content\":\"안녕하세요? \\nBuild_with_AI에 오신 것을 환영합니다.\", \"filename\":\"Build_with_AI_Test.pdf\", \"summary\":False})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rany0FPunZaL"
      },
      "outputs": [],
      "source": [
        "# PDF 읽기 도구 예시\n",
        "\n",
        "print(\"\\n\\n\", read_tool.invoke(\"Build_with_AI_Test.pdf\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 목록 도구 예시\n",
        "\n",
        "print(listdir_tool.invoke(input=\"\"))"
      ],
      "metadata": {
        "id": "bBAYyu8xYpM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDLWFnegnikc"
      },
      "outputs": [],
      "source": [
        "# 삭제 도구 예시\n",
        "\n",
        "delete_tool.invoke(\"Build_with_AI_Test.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uOvOZf3nKBn"
      },
      "outputs": [],
      "source": [
        "# LLM에게 도구 할당\n",
        "\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrMAISv-nKBo"
      },
      "outputs": [],
      "source": [
        "# 전체 메시지 중 마지막 메시지를 제외하고 최대 8개까지 단기 기억(history)으로 추출\n",
        "\n",
        "def shorterm_memory(state:State):\n",
        "\n",
        "    if len(state[\"messages\"]) > 8:\n",
        "        history = state[\"messages\"][-8:-1]\n",
        "    elif len(state[\"messages\"]) == 1:\n",
        "        history = \"\"\n",
        "    else:\n",
        "        history = state[\"messages\"][:-1]\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsqpQmxknKBo"
      },
      "outputs": [],
      "source": [
        "class HistoryChecker(BaseModel):\n",
        "    \"\"\"\n",
        "    이전의 대화 기록을 참고하여 질문에 대해 답변할 수 있는지 판단합니다.\n",
        "    답변할 수 있다면 \"yes\", 답변할 수 없다면 \"no\"를 반환합니다.\n",
        "    \"\"\"\n",
        "\n",
        "    yes_no : Literal[\"yes\", \"no\"] = Field(..., description=\"\"\"Use your previous conversation history to determine if you can answer your questions.\n",
        "    Return \"yes\" if you can answer, \"no\" if you can't answer.\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SsCloETnKBp"
      },
      "outputs": [],
      "source": [
        "# 히스토리 기반 답변 분기를 위한 함수 설정\n",
        "\n",
        "def history_check(state:State):\n",
        "\n",
        "    prompt = PromptTemplate.from_template(\"\"\"\n",
        "\n",
        "                이전의 대화 기록을 참고하여 질문에 대해 답변할 수 있는지 판단합니다.\n",
        "                답변할 수 있다면 \"yes\", 답변할 수 없다면 \"no\"를 반환합니다.\n",
        "\n",
        "                대화 기록 : {history}\n",
        "\n",
        "                질문 : {query}\n",
        "\n",
        "                \"\"\")\n",
        "\n",
        "    chain = prompt | history_checker\n",
        "\n",
        "    history = shorterm_memory(state)\n",
        "\n",
        "    result = chain.invoke({\"history\":history,\n",
        "                            \"query\":state[\"query\"]})\n",
        "\n",
        "    return result.yes_no"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDAlnV3dnKBp"
      },
      "outputs": [],
      "source": [
        "# LLM의 응답을 HistoryChecker 클래스 구조에 맞춰 파싱하도록 설정\n",
        "\n",
        "history_checker = llm.with_structured_output(HistoryChecker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx8dfRdlnKBp"
      },
      "outputs": [],
      "source": [
        "# 기억 기반 답변 노드\n",
        "\n",
        "def memory_chat(state:State):\n",
        "\n",
        "    prompt = PromptTemplate.from_template(\"\"\"\n",
        "\n",
        "                이전의 대화 기록을 참고하여 질문에 대해 답변하세요.\n",
        "                아래 대화 기록을 첨부합니다.\n",
        "                대화 기록을 통해 답변이 어렵다면 내부 지식을 참조하세요.\n",
        "\n",
        "                대화 기록 : {history}\n",
        "\n",
        "                질문 : {query}\n",
        "\n",
        "                \"\"\")\n",
        "\n",
        "\n",
        "    chain = prompt | llm\n",
        "\n",
        "    history = shorterm_memory(state)\n",
        "\n",
        "    result = chain.invoke({\"history\":history,\n",
        "                           \"query\":state[\"query\"]})\n",
        "\n",
        "    if len(state[\"tool_call\"]) == 0:\n",
        "        return {\"answer\":result.content,\n",
        "                \"messages\":result,\n",
        "                \"tool_call\":\"사용된 기록 없음.\"}\n",
        "    else:\n",
        "        return {\"answer\":result.content,\n",
        "                \"messages\":result}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLm8qqC7nKBq"
      },
      "outputs": [],
      "source": [
        "# 기억 기반 답변 분기 노드\n",
        "\n",
        "def history_node(state:State):\n",
        "    if len(state[\"messages\"]) == 1:\n",
        "        return {\"answer\":\"답변 없음\",\n",
        "                \"tool_call\":\"사용된 도구 없음\"}\n",
        "    else:\n",
        "        return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVxNiGN0nKBq"
      },
      "outputs": [],
      "source": [
        "# 도구 선택 노드\n",
        "\n",
        "def select(\n",
        "    state: State,\n",
        "):\n",
        "\n",
        "    prompt = PromptTemplate.from_template(\"\"\"\n",
        "\n",
        "                이전의 대화 기록을 참고하여 질문에 대해 답변하세요.\n",
        "                아래 대화 기록을 첨부합니다.\n",
        "                이전의 대화가 다음에 어떤 도구를 사용해야하는지 힌트가 될 수 있습니다. 꼭 참조하세요.\n",
        "                도구의 변화가 큰 결과를 가져올 수 있습니다.\n",
        "                들어온 메시지, 정답, 이전 기록을 모두 분석하여 가장 적절한 도구를 선택하세요.\n",
        "\n",
        "                대화 기록 : {history}\n",
        "\n",
        "                최근 사용한 도구 : {tool_name}\n",
        "\n",
        "                정답 : {answer}\n",
        "\n",
        "                질문 : {query}\n",
        "\n",
        "                \"\"\")\n",
        "\n",
        "    chain = prompt | llm_with_tools\n",
        "\n",
        "    history = shorterm_memory(state)\n",
        "\n",
        "    result = chain.invoke({\"history\" : history,\n",
        "                           \"tool_name\" : state[\"tool_call\"],\n",
        "                            \"answer\": state[\"answer\"],\n",
        "                            \"query\": state[\"query\"]})\n",
        "\n",
        "    if hasattr(result, \"tool_calls\") and len(result.tool_calls) > 0:\n",
        "        tool_calls = result.tool_calls\n",
        "\n",
        "        return {\"messages\": result,\n",
        "                \"tool_call\":tool_calls}\n",
        "    else:\n",
        "        return {\"messages\":AIMessage(content=f\"\"\"도구를 선택하지 못했습니다. 적절한 도구를 재선택하세요.\n",
        "                                        \"\"\"),\n",
        "                                    \"tool_call\":\"선택된 도구 없음\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6s0RyEfnKBq"
      },
      "outputs": [],
      "source": [
        "# 도구 실행 노드\n",
        "\n",
        "tool_node = ToolNode(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD-vMnQenKBq"
      },
      "outputs": [],
      "source": [
        "class AnswerChecker(BaseModel):\n",
        "    \"\"\"\n",
        "    정답 분류기입니다.\n",
        "\n",
        "    정답이 질문을 해결했는지 여부를 판단합니다.\n",
        "    질문을 해결하지 못했을 시 해결될 때까지 도구를 이용합니다.\n",
        "\n",
        "    질문을 해결했다면 \"end\", 해결하지 못했다면 \"tool\"을 반환합니다.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    end : Literal[\"end\", \"tool\"] = Field(..., description=\"\"\"You are the answer sorter.\n",
        "\n",
        "                                                                Determine if the correct answer has solved the question.\n",
        "                                                                If the question is not resolved, use the tool until it is resolved.\n",
        "\n",
        "                                                                Return \"end\" if you solved the question, or \"tool\" if you didn't.\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51lPyxhdnKBr"
      },
      "outputs": [],
      "source": [
        "# LLM의 응답을 AnswerChecker 클래스 구조에 맞춰 파싱하도록 설정\n",
        "\n",
        "answer_checker = llm.with_structured_output(AnswerChecker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyZTkR8fnKBr"
      },
      "outputs": [],
      "source": [
        "# 답변 확인 노드\n",
        "\n",
        "def response(state:State):\n",
        "\n",
        "    return {\"answer\":state[\"messages\"][-1]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBV4ASvjnKBr"
      },
      "outputs": [],
      "source": [
        "# 답변 완성 판단 분기 함수\n",
        "\n",
        "def answer_check(state:State):\n",
        "\n",
        "    prompt = PromptTemplate.from_template(\"\"\"\n",
        "    당신은 정답 분류기 어시스턴트입니다.\n",
        "\n",
        "    정답이 질문을 해결하였는지 여부를 판단합니다.\n",
        "    질문을 해결하지 못했다면 도구를 이용합니다.\n",
        "\n",
        "    질문을 해결하였다면 \"end\", 아니라면 \"tool\"을 반환합니다.\n",
        "\n",
        "    기존 History도 참고하여 답변하세요.\n",
        "\n",
        "    History : {history}\n",
        "\n",
        "    정답 : {answer}\n",
        "\n",
        "    질문 : {query}\n",
        "    \"\"\")\n",
        "\n",
        "    chain = prompt | answer_checker\n",
        "\n",
        "    history = shorterm_memory(state)\n",
        "\n",
        "    result = chain.invoke({\"history\" : history,\n",
        "                            \"answer\": state[\"answer\"],\n",
        "                            \"query\": state[\"query\"]})\n",
        "\n",
        "    return result.end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50EDu4mOnKBr"
      },
      "outputs": [],
      "source": [
        "# 그래프 정의\n",
        "\n",
        "graph_builder = StateGraph(State)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDdqjxeNnKBr"
      },
      "outputs": [],
      "source": [
        "# 노드 및 엣지 정의\n",
        "\n",
        "graph_builder.add_node(\"history_node\", history_node)\n",
        "graph_builder.add_node(\"memory_chat\", memory_chat)\n",
        "graph_builder.add_node(\"select\", select)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "graph_builder.add_node(\"response\", response)\n",
        "\n",
        "\n",
        "graph_builder.add_edge(START, \"history_node\")\n",
        "graph_builder.add_conditional_edges(\"history_node\",\n",
        "                            history_check,\n",
        "                            {\"yes\":\"memory_chat\",\n",
        "                             \"no\":\"select\"})\n",
        "graph_builder.add_edge(\"select\", \"tools\")\n",
        "graph_builder.add_edge(\"tools\", \"response\")\n",
        "graph_builder.add_edge(\"memory_chat\", \"response\")\n",
        "graph_builder.add_conditional_edges(\"response\",\n",
        "                                    answer_check,\n",
        "                                    {\"end\":END,\n",
        "                                    \"tool\":\"select\"});"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8Z71CK8nKBr"
      },
      "outputs": [],
      "source": [
        "# 메모리 설정 및 그래프 컴파일\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "graph = graph_builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ2jWtlinKBs"
      },
      "outputs": [],
      "source": [
        "# 그래프 시각화\n",
        "# 가끔 \"ReadTimeout: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)\"라는 에러가 발생\n",
        "# 시간 초과로 그래프 생성에 실패했다는 메시지일뿐 기능과는 관계없으니 진행해도 괜찮습니다.\n",
        "\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWcfKPoOnKBr"
      },
      "outputs": [],
      "source": [
        "# config 재생성 노드, config의 재사용은 고려되지 않음. 재사용한다면 변수에 할당하여 사용할 것\n",
        "\n",
        "def reset_config(limit=20):\n",
        "\n",
        "    thread_id=random.randint(1,999999)\n",
        "\n",
        "    config = RunnableConfig(recursion_limit=limit, configurable={\"thread_id\": thread_id})\n",
        "\n",
        "    return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07_bBQgHnKBs"
      },
      "outputs": [],
      "source": [
        "# 출력 함수 정의\n",
        "# mode = \"values\" : 상태의 키, 값의 형태로 반환\n",
        "# mode = \"updates\" : 업데이트되는 값만 반환\n",
        "\n",
        "def streaming(query, config, mode=\"values\"):\n",
        "\n",
        "    result = graph.stream({\"messages\":(\"user\", query),\n",
        "                         \"query\":query}, config=config, stream_mode=mode)\n",
        "\n",
        "    if mode == \"values\":\n",
        "        for step in result:\n",
        "            for k, v in step.items():\n",
        "                if k == \"messages\":\n",
        "                    v[-1].pretty_print()\n",
        "    elif mode == \"updates\":\n",
        "        for step in result:\n",
        "            for k,v in step.items():\n",
        "                print(f\"\\n\\n=== {k} ===\\n\\n\")\n",
        "                print(v)\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI6E1DvhnKBs"
      },
      "outputs": [],
      "source": [
        "config = reset_config()\n",
        "\n",
        "query = \"1+1은 뭔가요?\"\n",
        "\n",
        "streaming(query, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KUL2l9hIHsf"
      },
      "outputs": [],
      "source": [
        "config = reset_config()\n",
        "\n",
        "query = \"피엔티라는 회사에 대해 조사해주세요. 잘 정리된 보고서를 제공해주십시오. pdf파일로 받기를 희망합니다.\"\n",
        "\n",
        "streaming(query, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJQuCIX4IHsf"
      },
      "outputs": [],
      "source": [
        "query = \"피엔티의 경쟁사에 대한 정보도 정리된 보고서를 작성해주세요.\"\n",
        "\n",
        "streaming(query, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk2rpxQAnKBs"
      },
      "outputs": [],
      "source": [
        "config = reset_config()\n",
        "\n",
        "query = \"현재 폴더의 pdf로 이루어진 파일 모두 삭제해줘\"\n",
        "\n",
        "streaming(query, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3rAme8unKBt"
      },
      "outputs": [],
      "source": [
        "config = reset_config()\n",
        "\n",
        "code = \"\"\"\n",
        "아래 코드 실행시켜주세요.\n",
        "\n",
        "```python\n",
        "\n",
        "result = 0\n",
        "\n",
        "for i in range(20):\n",
        "    print(f\"{i+1}번째 출력: \", i+1)\n",
        "    result += i\n",
        "\n",
        "print(\"최종 결과: \", result)\n",
        "\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "streaming(code, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjIVlnOMnKBt"
      },
      "outputs": [],
      "source": [
        "config = reset_config()\n",
        "\n",
        "streaming(\"\"\"\n",
        "          모두의연구소는 어떤 곳이야?\n",
        "          깔끔하게 정리해서 레포트로 만들어줘.\n",
        "          레포트의 형식은 pdf로 저장해주면 돼.\n",
        "          이름은 \"모두의연구소_레포트.pdf\"로 해줘.\"\"\", config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = reset_config()\n",
        "\n",
        "query = \"'모두의연구소_레포트.pdf'라는 파일 삭제해줘\"\n",
        "\n",
        "streaming(query, config)"
      ],
      "metadata": {
        "id": "SBSbPqbjkUWv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Modulabs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}